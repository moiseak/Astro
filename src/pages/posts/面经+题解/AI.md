---
layout: /src/layouts/MarkdownPostLayout.astro
title: AI
description: AI相关面试题
pubDate: 2025-05-03
---
## RAG（Retrieval-Augmented Generation）

RAG 先检索向量数据库或知识库中的相关文档，再将其作为上下文注入生成模型，提升响应准确度与时效性 

- **优势**：实时接入新知识、降低模型漏报与幻觉、模型规模可更小


## 大模型是怎么训练出来的？

首先需要先准备数据，数据来源要广而杂，之后会对数据进行清理。之后是选择学习框架，目前主流大模型（如 GPT、LLaMA）都基于**Transformer 架构**，这是一种能高效处理序列数据（如文本、语音）的算法结构。

**Transformer 的核心：注意力机制**  。它能让模型在处理数据时 “聚焦重点”。比如理解句子 “他喜欢苹果，因为它很甜”，注意力机制会让模型知道 “它” 指代 “苹果”，而不是 “他”。这解决了传统模型处理长文本时 “记不住前文” 的问题。

训练过程：让模型 “从不会到会” 的迭代

这是最核心的环节，本质是让模型通过 “试错” 不断优化，最终能输出符合预期的结果。主要分为**预训练**和**微调**两大阶段。

| 阶段  | 目标     | 数据特点  | 训练方式  |
| --- | ------ | ----- | ----- |
| 预训练 | 掌握通用能力 | 海量无标注 | 无监督学习 |
| 微调  | 适配具体任务 | 少量有标注 | 有监督学习 |

